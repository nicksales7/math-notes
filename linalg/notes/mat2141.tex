\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

\newcommand{\incfig}[1]{%
    \def\svgwidth{8cm}
    \import{./figures/}{#1.pdf_tex}
}

% Document settings
\geometry{a4paper, margin=1in}
\setlength\parindent{0pt}
\setlength\parskip{1em}

\title{MAT 2141 Notes}
\author{Nicholas Sales}
\date{} % This will automatically insert today's date

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Linear Maps}

\subsection{Definiton}

\textbf{Definition 1.1.1:} If $V$ and $W$ are vector spaces over the same field $F$, and $T : V \rightarrow W$ is a mapping, we say $T$ is a linear mapping if the following hold:
\begin{gather*}
    T(u + v) = T(u) + T(v) \;\; \forall u,v \in V \\
    T(cv) = cT(v) \;\; \forall v \in V, \; \forall c \in F
\end{gather*}
\textbf{Example 1.1.1:} Let $V = C^{\infty}(\mathbb{R}), \; F = \mathbb{R}$, and define $S : C^{\infty}(\mathbb{R}) \rightarrow C^{\infty}(\mathbb{R})$ by $(Sf)(x) = \int_{0}^{x} f(t) dt, \; \forall x \in \mathbb{R}$. \\
\vspace{0.1cm} \\
Let $f, g \in V$ (i.e. $f$ and $g$ are infinitely differentiable functions) and let $c \in F$. Using the properties of integration, we can show additivity and homogeneity hold:
\begin{gather*}
    S(f + g)(x) = \int_{0}^{x} f(t) + g(t) dt = \int_{0}^{x} f(t) dt + \int_{0}^{x} g(t) dt = (Sf)(x) + (Sg)(x) \\
    S(cf)(x) = \int_{0}^{x} cf(t) dt = c \int_{0}^{x} f(t) dt = c(Sf)(x)
\end{gather*}
Since $S$ satisfies the properties of additivity and homogeneity, we conclude $S$ is a linear map. QED.

\subsection{Kernel and Image}

\textbf{Definiton 1.2.1:} If $T : V \rightarrow W$ is a linear map, then $Ker(T) \subseteq V$ gets mapped to the zero vector under $T$ and $Im(T) \subseteq W$ is where vectors from $V$ can actually get mapped to under $T$ (i.e. range):
\begin{gather*}
    Ker(T) = T^{-1}(\vec{0}) = \{x \; | \; Tx = \vec{0}\} \\
    Im(T) = T(V) = \{Tx \; | \; x \in V\}
\end{gather*}
\textbf{Theorem 1.2.1:} Suppose $T : V \rightarrow W$ is a linear mapping, $T$ is injective iff $Ker(T) = \{\vec{0}\}$. \\ 
\vspace{0.1cm} \\
We prove this in two directions. Assume $T$ is injective and let $u,v \in V$. Since $0v = \vec{0}$ for any vector $v$ and $T(0v) = 0T(v)$ by the property of linear maps, it follows a linear mapping always maps the zero vector to the zero vector. Since for any injective mapping we have $T(u) = T(v) \implies u = v$, it follows if $T(u) = T(v) = \vec{0}$, then $u = v = \vec{0}$ and thus $Ker(T) = \{\vec{0}\}$. \\
\vspace{0.1cm} \\
We now prove this in the other direction. Assume $Ker(T) = \vec{0}$. We want to show if $T(u) = T(v)$ for any $u,v \in V$ then $u = v$. If $T(u) = T(v)$, it follows $T(u) - T(v) = \vec{0}$ which simplifies to $T(u - v) = \vec{0}$ by the property of linear maps. Since $Ker(T) = \{\vec{0}\}$ it follows that $T(u - v) = \vec{0} \implies u - v = \vec{0} \implies u = v$. Thus $T$ is injective. Since we have proved both implications this completes the proof. QED.

\newpage

\subsection{Vector Spaces of Linear Maps}

\textbf{Definition 1.3.1:} If $V, W$ are vector spaces over a field $F$, we define the set of linear maps from $V$ to $W$:
\begin{equation*}
    \mathcal{L}(V,W) = \{T : V \rightarrow W \; | \; T \text{ is linear}\}
\end{equation*}
If $V = W$, we simply write $\mathcal{L}(V)$. \\
\textbf{Definition 1.3.2:} If $A, B,$ and $C$ are sets with mappings $T : A \rightarrow B$ and $S : B \rightarrow C$, we can compose the mappings to get $ST : A \rightarrow C$ (i.e. $S \circ T$) defined by:
\begin{equation*}
    (ST)a = S(Ta) \;\; \forall a \in A
\end{equation*}
\textbf{Theorem 1.3.1:} The composition of linear maps is itself a linear map (i.e. $T \in \mathcal{L}(U,V)$ and $S \in \mathcal{L}(V,W) \implies ST \in \mathcal{L}(U,W)$). \\
\vspace{0.1cm} \\
Let $T \in \mathcal{L}(U,V)$ and $S \in \mathcal{L}(V,W)$ be linear maps over a field $F$ with $u,v \in U$ and $c \in F$. By the definition of composition we have:
\begin{gather*}
    (ST)(u + v) = S(T(u + v)) \\
    (ST)(cu) =  S(T(cu))
\end{gather*}
By the property of $T$ being a linear map we get:
\begin{gather*}
    S(T(u + v)) = S(T(u) + T(v)) \\
    S(T(cu)) = S(cT(u))
\end{gather*}
And further, by the property of $S$ being a linear map we get:
\begin{gather*}
    S(T(u) + T(v)) = S(T(u)) + S(T(v)) \\
    S(cT(u)) = cS(T(u))
\end{gather*}
Thus, $ST$ satisfies additivity and homogeneity and is therefore a linear map. QED.

\subsection{Isomorphisms}

\textbf{Definition 1.4.1:} If $T : V \rightarrow W$ is a bijective linear map, we say $T$ is an isomorphism. We also say that $V$ is isomorphic to $W$ and write $V \cong W$. \\
\textbf{Theorem 1.4.1:} Suppose $V$ and $W$ are vector spaces. Further, assume $T : V \rightarrow W$ is an isomorphism. It follows that $T^{-1} : W \rightarrow V$ is also linear and hence an isomorphism. \\
\vspace{0.1cm} \\
Assume $T : V \rightarrow W$ is an isomorphism. Since it follows that $T$ is bijective, it is invertible. Let $u',v' \in W$. It follows there exists some $u,v \in V$ such that $T(u) = u'$ and $T(v) = v'$. By the property of $T$ being linear, we have:
\begin{gather*}
    T^{-1}(u' + v') = T^{-1}(T(u) + T(v)) = T^{-1}(T(u+v)) = u + v = T^{-1}(u') + T^{-1}(v')\\
    T^{-1}(cu') = T^{-1}(cT(u)) = T^{-1}(T(cu)) = cu = cT^{-1}(u')
\end{gather*}
Thus $T^{-1}$ preserves additivity and homogeneity and is therefore a linear map. And moreover, since it is the inverse of a bijective mapping, it is also bijective, and thus an isomorphism. QED. 

\newpage

\textbf{Theorem 1.4.2:} Isomorphisms are an equivalence relation. In other words, if $U,V,W$ are all vector spaces over the same field $F$, the following properties hold:
\begin{gather*}
    U \cong U \\ 
    U \cong V \implies V \cong U \\
    U \cong V \text{ and } V \cong W \implies U \cong W
\end{gather*}
This proof is left to the non-existent reader.

\section{Structure of Vector Spaces}

\subsection{Spans and Generating Sets}

\textbf{Theorem 2.1.1:} Suppose $A$ is a nonempty subset of a vector space $V$ and that $W$ is a subspace of $V$. Then $Span(A)$ is the smallest subspace of $V$ containing $A$. In other words:
\begin{gather*}
    A \subseteq Span(A) \\
    A \subseteq W \implies Span(A) \subseteq W
\end{gather*}
Let $\alpha \in A$. Since $Span(A)$ is the set of all linear combinations of vectors in $A$, it follows the linear combination $1 \alpha = \alpha \in Span(A)$. Thus $A \subseteq Span(A)$. \\
\vspace{0.1cm} \\
Assume $A \subseteq W$ and $w \in Span(A)$. It follows we can write $w = c_1 a_1 + \dots + c_n a_n$ for some $c_1, \dots, c_n \in F$ and $a_1, \dots, a_n \in A$. Since $W$ is a vector space, it satisifies closure properties under addition and scalar multiplication. Further, since $A \subseteq W$, it follows that $a_i + a_j \in W$ and $c_i a_i \in W$ for any $a_i, a_j \in A$ and $c_i \in F$. Therefore, it must follow that $w = c_1 a_1 + \dots + c_n a_n \in W$ and we can conclude $Span(A) \subseteq W$. QED. \\
\textbf{Definition 2.1.1:} If $V$ is a vector space with $A \subseteq V$, we say that $Span(A)$ is the subspace of $V$ generated by $A$, and thus $A$ is the generating set for $Span(A)$. We say that $A$ generates $Span(A)$, so it follows if $Span(A) = V$, we say $A$ generates $V$. \\
\textbf{Theorem 2.1.2:} Suppose $T : V \rightarrow W$ is a linear map and $A$ is a subset of $V$. Then $SpanT(A) = T(Span(A))$. \\
\vspace{0.1cm} \\
Let $a_1 , \dots , a_n$ be the vectors in $A$ and let $c_1 , \dots , c_n \in F$. We prove this in two directions. First, assume $c_1 T(a_1) + \dots + c_n T(a_n) \in SpanT(A)$. We can use the properties of linear maps to show:
\begin{align*}
    c_1 T(a_1) + \dots + c_n T(a_n) &= T(c_1 a_1) + \dots + T(c_n a_n) \\
    &= T(c_1 a_1 + \dots + c_n a_n) \in T(Span(A))
\end{align*}
Thus it follows $SpanT(A) \subseteq T(Span(A))$. In the other direction, assume $T(c_1 a_1 + \dots + c_n a_n) \in T(Span(A))$. Again, by the properties of linear maps:
\begin{align*}
    T(c_1 a_1 + \dots + c_n a_n) &= T(c_1 a_1) + \dots + T(c_n a_n) \\
    &= c_1 T(a_1) + \dots + c_n T(a_n) \in SpanT(A)
\end{align*}
Thus $T(Span(A)) \subseteq SpanT(A)$. We therefore conclude $SpanT(A) = T(Span(A))$. QED.

\newpage

\subsection{Linear Dependence/Independence}

\textbf{Definition 2.2.1:} Suppose $V$ is a vector space and $v_i \in V$ are a list vectors in $V$ and $c_i \in F$ are a list of scalars in $F$ with $i \in \mathbb{N}$. If at least one of the $c_i$ is nonzero and:
\begin{equation*}
    \sum_{n = 1}^{i} c_n v_n = \vec{0}
\end{equation*}
We say that the list of vectors is linearly dependent. If the $c_i$ must all be zero for the equation to hold true, the list is said to be linearly independent. \\
\textbf{Definition 2.2.2:} If $A$ is any (possibily infinite) set of vectors in a vector space $V$, if every finite list of distinct vectors in $A$ is linearly independent, then $A$ is linearly independent. However, if there is some finite list of vectors in $A$ that is linearly dependent, then $A$ is linearly dependent. \\
\textbf{Theorem 2.2.1:} Suppose $V$ is a vector space over a field $F$ with $v_1 , \dots, v_n \in V$. Define the linear map $T : F^n \rightarrow V$ by $T(a_1, \dots, a_n) = a_1 v_1 + \dots + a_n v_n$. The list of vectors $v_1, \dots, v_n$ are linearly dependent iff $T$ is not injective. \\
\vspace{0.1cm} \\
We prove this in two directions. First, assume $v_1 , \dots , v_n$ are linearly dependent. Since the vectors are linearly dependent, there exists some $a_1, \dots, a_n \in F$, not all zero, such that $a_1 v_1 + \dots + a_i v_i + \dots + a_n v_n = \vec{0}$. It follows that $(a_1 , \dots , a_i , \dots a_n) \in Ker(T) \implies Ker(T) \neq \{\vec{0}\}$. Thus $T$ cannot be injective. \\
\vspace{0.1cm} \\
Now, assume $T$ is not injective. It follows there is some $(a_1 , \dots , a_n), (b_1 , \dots , b_n) \in F^n$ such that $T(a_1, \dots, a_n) = T(b_1 , \dots , b_n)$ and $(a_1 , \dots , a_n) \neq (b_1 , \dots , b_n)$. We thus have:
\begin{align*}
    T(a_1 , \dots , a_n) = T(b_1 , \dots , b_n) &\implies a_1 v_1 + \dots + a_n v_n = b_1 v_1 + \dots + b_n v_n \\
                                                &\implies (a_1 - b_1) v_1 + \dots + (a_n - b_n) v_n = \vec{0}
\end{align*}
Since $(a_1 , \dots , a_n) \neq (b_1 , \dots , b_n)$, it follows that $(a_1 - b_1 , \dots , a_n - b_n) \in F^n$ is nonzero, meaning at least one component $a_i - b_i \neq \vec{0}$. Since $(a_1 - b_1 , \dots , a_n - b_n) \in Ker(T)$ and is nonzero, the vectors $v_1 , \dots , v_n$ are linearly dependent. QED. \\
\textbf{Theorem 2.2.2:} Suppose $T : V \rightarrow W$ is an injective linear map and $v_1 , \dots , v_n$ are linearly independent vectors in $V$, then $T(v_1) , \dots , T(v_n)$ are linearly independent vectors in $W$. \\
\vspace{0.1cm} \\
Let $c_1 , \dots , c_n \in F$. Since $T$ is a linear map, we have:
\begin{equation*}
    c_1 T(v_1) + \dots + c_n T(v_n) = \vec{0} \implies T(c_1 v_1 + \dots + c_n v_n) = \vec{0}
\end{equation*}
Further, since $T$ is injective and $v_1 , \dots , v_n$ are linearly independent:
\begin{equation*}
    T(c_1 v_1 + \dots + c_n v_n) = \vec{0} \implies c_1 v_1 + \dots + c_n v_n = \vec{0} \implies c_1 = \dots = c_n = 0
\end{equation*}
QED.
\end{document}
